\section{Locality Sensitive Hashing}
\label{section:A-LSH}
The technique of locality sensitive hashing was introduced by \cite{gionis1999similarity} to solve the problem of approximate nearest neighbour search. The LSH-based hashing schemes try to put similar points into the same bucket. Hence, to search for points which are `similar' to a given point $x \in X$, one only needs to search within the same hash bucket instead of searching within the whole set $X$. Next, we describe a generic hashing based algorithm. 

\begin{definition}[LSH \cite{charikar2002similarity}]
\label{defn:LSH}
Given a set $X$ and a similarity function $f:X \times X \rightarrow [0, 1]$. Given a class of hash functions $H = \{h_1, h_2, \ldots \}$. An LSH w.r.t $X$ and $f$ is a probability distribution over $H$ such that for each $x, y \in \mc X$, $$\underset{h \in H}{\mb P}\enspace [h(x) = h(y)] = f(x, y)$$
\end{definition}
\noindent Some common examples of an LSH schemes are minhash scheme w.r.t jaccard similarity measure \cite{broder2000min, broder1997resemblance}, simhash scheme w.r.t hamming similarity measure \cite{charikar2002similarity}. We describe a generic locality sensitive hashing procedure in Alg. \ref{alg:LSH}. Observe that, Alg. \ref{alg:LSH} outputs $s$ different partitions of $X$. Assume without loss of generality, that a point $x$ lies in the blocks $B_{11} \in P_1, B_{21} \in P_2, \ldots, B_{s1} \in P_s$. Now, to search for points $y$ which are similar to $x$, we only need to search within the blocks $B_{i1}$. We say that these points lie in the same `bucket' as $x$. We say that $b(x, y) = 1$ if and only if $x$ and $y$ lie together in the same block in atleast one of the partitions.

\begin{algorithm}
\caption{A generic LSH based hashing algorithm \cite{indyk1998approximate,charikar2002similarity}}
\label{alg:LSH}

\Indp\KwIn{A set $X$, a similarity function $f$, a class of hash functions $H$ over $X$, integers $r, s$ and a perfect hash function $p$ over the domain $\mb N^r$.}
\KwOut{Partitions $P_1, \ldots, P_s$ of the set $X$.}
\vspace{0.1in} Let $D$ be a distribution over $H$ which satisfies Defn. \ref{defn:LSH} and let $k = r s$.\\
Sample hash functions $h_1, \ldots, h_k$ iid using $D$.\\
 Group the hash functions into $s$ bands. Each band contains $r$ hash functions. \\
For all $x$ and $1\le i \le s$,  let $g_i(x) = (h_{(i-1)s + 1}(x), \ldots, h_{ir}(x))$. That is, $g_i(x)$ represents the $i^{th}$ signature of $x$. \\
For all $1 \le i \le s$, obtain partitions $P_i$ of $\mc X$. That is, $P_i = \{B_{i1}, \ldots, B_{im_i}\}$ where each $B_{ij} = \{x : p(g_i(x)) = b_{ij}\}$ for some $b_{ij}$.\\
Output $\{P_1, \ldots, P_s\}$.
\end{algorithm}



Hence, to search for points which are similar to $x$, we need to search over $y$ such that $b(x, y) = 1$. Our sampling procedure will sample pairs from the set $\mc Q := \{(x, y) : b(x, y) = 1\}$ (details in the next section). Hence, a requirement from the hashing scheme is that we should be able to construct the set $\mc Q$ in linear time. Thm. \ref{thm:hashTimeComplexity} shows that this is indeed the case. 

\begin{restatable}{thrm}{hashTimeComplexity}
\label{thm:hashTimeComplexity}
Given $X$, a similarity function $f$, a class of hash functions $H$, integers $r, s$ and perfect hash function $p$. Alg. \ref{alg:LSH} runs in $O(|X|rs \max_{ij}|B_{ij}|)$.
\end{restatable}
\begin{proof}
Let $n = |X|$. Sampling $k$ different hash function takes $rs$ time. Computing the signatures for all $x$ takes $nrs$ time. Obtaining the different partition takes $ns$ time. Now, computing $R_x$ for all $x$ takes time $t = \sum_{i = 1}^b \sum_{j=1}^{m_j} |B_{ij}|^2$. Now, we know that for all $i$, $\sum_{j=1}^{m_j} |B_{ij}| = n$. Hence, $\sum_{j=1}^{m_j} |B_{ij}|^2 \le \max_j B_{ij} \enspace \sum_{j=1}^{m_j} |B_{ij}| = n \max_j{B_{ij}}$. Hence, $t \le n s \max_{ij} |B_{ij}|$.  
\end{proof}

\noindent We see that the running time is dependent upon the block sizes. If the maximum block size is a constant then the hashing scheme runs in linear time. Even when the block sizes are bounded by $\log n$, then the scheme runs in $O(n \log n)$.  Next, we show that a if $d(x, y) \le \lambda$ then $(x, y) \in \mc Q$ with very high probability. Also, if $d(x, y) > O(2\lambda)$ then with high probability $(x, y) \not\in \mc Q$.   

\begin{restatable}{thrm}{similarInSame}
\label{thm:similarInSame}
Given a set $X$, a distance function $d: X \times X \rightarrow [0, 1]$, a class of hash functions $H$, threshold parameter $\lambda$ and a parameter $\delta$. Let the similarity function be $f(x, y) := 1 - d(x, y)$ and let $\mc A$ be a generic LSH based algorithm (Alg. \ref{alg:LSH}) which outputs partitions $P_1, \ldots, P_s$. Let $b(x, y) = 1$ iff $x, y$ are together in atleast one of these partitions. 
\noindent Choose $r, s$ such that $\frac{1}{2\lambda} < r < \frac{1}{-\ln(1-\lambda)} $ and $s =  \lceil 2.2\ln(\frac{1}{\delta})\rceil$. Define $\delta' := s\ln(1+\delta)$. Then for $(x, y) \in \mc X^2$
\begin{itemize}
	\item If $d(x, y) \le \lambda$ then 
	\begin{align*}
		& \underset{h \in H}{\mb P} \enspace[\enspace b(x, y) = 1 ] \enspace > \enspace 1 - \delta
	\end{align*}
	\item If $d(x, y) > 2\lambda\ln\big(1+\frac{1}{\delta}\big)$ then 
	\begin{align*}
		& \underset{h \in H}{\mb P} \enspace[\enspace b(x, y) = 1 ] \enspace < \enspace \delta'
	\end{align*}
\end{itemize}
\end{restatable}
\begin{proof}
Observe that
\begin{align*}
  & \mb P[b(x, y) = 0] = \mb P \enspace  [ \underset{i=1}{\cap^s} g_i(x) \neq g_i(y)] \\
  &=\prod_i\Big(1 - \prod_{j=1}^r \mb P[ h_{(i-1)r+j}(x) = h_{(i-1)r+j}(y)]\Big)\\
  & = \prod_{i=1}^s(1 - \prod_{j=1}^r f(x, y)) = (1-f(x, y)^r)^s
\end{align*}
Consider the case when $d(x, y) \le \lambda$. From the choice of $s$, we know that $s \ge 2.2\ln(1/\delta) \implies s \ge \frac{\ln(1/\delta)}{1-\ln(e-1)}\iff 1-\frac{1}{e} \le \delta^{1/s}$. From the choice of $r$, we know that $r < \frac{1}{-\ln(1-\lambda)} \iff r \ln(\frac{1}{1-\lambda}) < 1 \iff (1-\lambda)^r > \frac{1}{e}$.  Hence,   then we have that $\mb P[b(x, y) = 0]  = (1 - (1-d(x, y))^r)^s \le (1 - (1-\lambda)^r)^s < \delta$. This proves the first part of the theorem. 

For the second part, consider the case when $d(x, y) > \lambda'$ where $\lambda'$ is such that $\ln(1+1/\delta) = \frac{\lambda'}{2\lambda}$. 
\begin{flalign*}
  & \frac{1}{2\lambda} < r \iff \frac{\ln(1+1/\delta)}{\lambda'} < r. \enspace\text{Now, $\lambda' \le \ln\Big(\frac{1}{1-\lambda'}\Big)$. Hence, }&\\
  &\implies \frac{\ln(1+1/\delta)}{\ln(\frac{1}{1-\lambda'})} < r \iff r\ln(1-\lambda') < \ln(\frac{\delta}{1+\delta}) &\\
  &\iff 1-(1-\lambda')^r > e^{-\ln(1+\delta)} = e^{-\delta'/s} > (1-\delta')^{1/s}&\\
  &\implies \mb P[b(x, y) = 0] > (1-(1-\lambda')^r)^s > 1-\delta'&
\end{flalign*}
Now, the only thing that remains to be shown is that we can choose an integer $r$ satisfying the conditions of the theorem. Consider the function $f(x) = -\frac{1}{2x} - \frac{1}{\ln(1-x)}$. Using elementary analysis, we see that for $x \rightarrow 0, f(x) \rightarrow \infty$. Infact, for $x \le 0.32, f(x) > 1$. Hence, for $\lambda \le 0.32$, $r$ satisfying the conditions of the theorem exists.
\end{proof}

\section{Sample and query complexity of RCC using $\mc P_{12}$}
\begin{restatable}{thrm}{sampleComplexityLSH}
Given metric space $(X, d)$, a class of clusterings $\mc F$ of $X$ and a threshold parameter $\lambda$. Given $\epsilon, \delta \in (0, 1)$ and a $C^*$-oracle. Let $d$ be $(\alpha, \beta)$-informative w.r.t $C^*$ and $\lambda$ and let $X$ satisfy $\gamma$-skewed property. Let $\mc A$ be the ERM-based approach as described in Alg. \ref{alg:ERM} (where the LSH-based scheme is $\zeta$-tight) and $\hat C$ be the output of $\mc A$. If  
\label{thm:sampleComplexityLSH}
\begin{align}
  &m_-, m_+ \enspace \ge a\frac{\vcdim({\mc F}) + \log(\frac{2}{\delta})}{\epsilon^2} 
\end{align}
where $a$ is a global constant then with probability atleast $1-\delta - \exp\big(\frac{-2\nu^2(1-\alpha)^2|X_2^+|^2}{49}\big)$ (over the randomness in $\mc A$), we have that $$L_{C^*}(\hat C) \enspace\le\enspace \min_{\mc C \in \mc F} L_{C^*}(C) + 3\alpha + \zeta + \epsilon + \nu$$
\end{restatable}
\begin{proof}
Let $T_0$ be the distribution induced by $\mc P_0$ and $T_1$ be the distribution induced by $\mc P_1$. Using Thm. \ref{thm:uniformConvergence}, we know that if $m_+ > a\frac{\vcdim({\mc F}) + \log(\frac{1}{\delta})}{\epsilon^2}$ then with probability atleast $1-\delta - e^{-2\nu^2(1-\alpha)^2|X_2^+|^2}$, we have that for all $C$
\begin{align*}
  &|\hat P(C) - \underset{(x,y) \sim T_1}{\mb P}[C(x, y) = 0]| \le \epsilon\\
  \implies &\hat P(C) \le \epsilon + (1+2\nu)(1+2\alpha)L_{P^+}(C) \enspace\text{ and}\\
  &L_{P^+}(C) - \epsilon - \alpha -\zeta - \nu \le \hat P(C) \numberthis \label{eqn:eHatLSH}
\end{align*}
Note that we obtain upper and lower bounds for $\underset{(x,y) \sim T_1}{\mb P}[h(x, y) = 0]$ using Thm. \ref{thm:posDistributionLSH}. Similarly, if $m_- > a\frac{\vcdim({\mc F}) + \log(\frac{1}{\delta})}{\epsilon^2}$, then with probability atleast $1-\delta$, we have that for all $h$,
\begin{align*}
  &|\hat N(C) - \underset{(x,y) \sim T_0}{\mb P}[C(x, y) = 1]| \le \epsilon\\
  \implies &\hat N(C) \le \epsilon + L_{P^-}(C) \enspace\text{ and } L_{P^-}(C) -\epsilon \le \hat N(C) \numberthis\label{eqn:gHatLSH}
\end{align*}

\noindent Combining Eqns. \ref{eqn:eHatLSH} and \ref{eqn:gHatLSH}, we get that with probability atleast $1-2\delta-e^{-2\nu^2(1-\alpha)^2|X_2^+|^2}$, we have that for all $C \in {\mc F}$
\begin{align*}
  \hat L(C) &\le \mu [\epsilon + (1+2\nu)(1+2\alpha)L_{P^+}(C)]\\
  &+ (1-\mu)(\epsilon + L_{P^-}(C))\\
  &\le L_{C^*}(C) + \epsilon + 2\alpha + 2\nu + 4\alpha\nu\\
  \text{And} \enspace \hat L(C) &\ge \mu(L_{P^+}(h) -\epsilon - \alpha - \nu-\zeta) + (1-\mu)(L_{P^-}(h) - \epsilon) \\
  &\ge L_{C^*}(C) - \epsilon - \alpha - \nu - \zeta
\end{align*}
Now, let $\hat C$ be the output of $\mc A$ and let $\hat C^*$ be $\argmin_{C \in {\mc F}} L_{C^*}(C)$. Then, we have that
\begin{align*}
  L_{C^*}(\hat C) &\le \hat L(\hat C) + \alpha + \epsilon + \nu +\zeta \le \hat L(\hat C^*) + \alpha + \epsilon + \nu +\zeta\\
  &\le L_{C^*}(\hat C^*) + 2\epsilon + 3\alpha + 3\nu + 4\alpha\nu + \zeta
\end{align*}

Choosing $\epsilon = \frac{\epsilon}{2}$ and $\delta = \frac{\delta}{2}$ and $\nu = \frac{\nu}{7}$ throughout gives the result of the theorem,
and this completes the proof of the theorem.
\end{proof}

Finally, we analyse the query complexity of our approach. That is the number of queries that our algorithm makes to the $C^*$-oracle. Our algorithm makes queries during the sampling procedure. We see that to sample $m_-$ negative and $m_+$ positive pairs the number of queries is `close' to $m_+ + m_-$ with very high probability. Thus, the number of `wasted' queries is small.  

\begin{restatable}{thrm}{queryComplexityLSH}[Query Complexity]
\label{thm:queryComplexity}
Let the framework be as in Thm. \ref{thm:sampleComplexityLSH}. With probability atleast $1-\exp\big(-\frac{\nu^2m_-}{4}) - \exp\big(-\frac{\nu^2m_+}{4}\big) - \exp(-\nu^2(1-\alpha)^2|\mc X_+^2|^2)$ over the randomness in the sampling procedure, the number of same-cluster queries $q$ made by $\mc A$ is  
$$q \le (1+\nu)\bigg(\frac{m_-}{(1-\gamma)} + \frac{m_+}{\beta(1-\zeta-\nu)}\bigg)$$
\end{restatable}

\begin{proof}
The number of queries made to sample the set $S_g$ is $q_g = m_g$. Let $q_+$ denote the number queries to sample the set $S_+$. Using Lemma \ref{lemma:posQueriesLSH}, we know that $\mb E[q_+] \le \frac{1}{\beta(1-\zeta-\nu)}$ with probability atleast $1-\exp(-\nu^2(1-\alpha)^2|\mc X_+^2|^2)$ over the randomness in the hashing procedure. Given that the expectation is bounded as above, using Thm. \ref{thm:geometricRV}, we get that $q_+ \le \frac{(1+\nu)m_+}{\beta(1-\zeta-\nu)}$ with probability atleast $1-\exp(\frac{-\nu^2m_+}{4})$. Similarly, combining Lemma \ref{lemma:negQueries} with Thm. \ref{thm:geometricRV}, we get that with probability atleast $1-\exp(\frac{-\nu^2m_-}{4})$, $q_- \le \frac{(1+\nu)m_-}{(1-\gamma)}$.
\end{proof}