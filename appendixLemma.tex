\def\COMPLETE{}

\section{Concentration inequalities}
\label{appendixsection:conIneq}

\begin{theorem}[Generalized Hoeffding's Inequality (e.g., \cite{ashtiani2015dimension})]
\label{thm:genHoeff}
Let $X_1, \ldots. X_n$ be i.i.d random vectors in some Hilbert space such that for all $i$, $\|X_i\|_2 \le R$ and $E[X_i] = \mu$. If $n > c\frac{\log(1/\delta)}{\epsilon^2}$, then with probability atleast $1-\delta$, we have that
$$\Big\|\mu - \frac{1}{n}\sum X_i\Big\|_2^2 \le R^2\epsilon$$ 
\end{theorem}

\begin{theorem}[Multiplicative Chernoff bound \cite{mitzenmacher2005probability}]
\label{thm:chernoff}
Let $X_1, \ldots, X_n$ be i.i.d random variables in $\{0, 1\}$ such that $\mu = E[X_i]$. Let $X = \frac{\sum X_i}{n}$. Then for any $0 < \epsilon < 1$
$$P\big[ \enspace X > (1+\epsilon) \mu\enspace\big] \enspace\le\enspace \exp\bigg(\frac{-\epsilon^2\mu n}{3}\bigg)$$
\end{theorem}

\begin{theorem}[Multiplicative Chernoff bound \cite{mitzenmacher2005probability}]
\label{thm:chernoff2}
Let $X_1, \ldots, X_n$ be i.i.d random variables in $\{0, 1\}$ such that $\mu = E[X_i]$. Let $X = \frac{\sum X_i}{n}$. Then for any $0 < \epsilon < 1$
$$P\big[ \enspace X < (1-\epsilon) \mu\enspace\big] \enspace\le\enspace \exp\bigg(\frac{-\epsilon^2\mu n}{2}\bigg)$$
\end{theorem}

\section{Learning theory results}
\begin{theorem}[Vapnik and Chervonenkis \cite{vapnik2015uniform}]
\label{thm:vceapprox}
Let $X$ be a domain set and $D$ a probability distribution over $X$. Let $H$ be a class of subsets of $X$ of finite VC-dimension $d$. Let $\epsilon, \delta \in (0,1)$. Let $S \subseteq X$ be picked i.i.d according to $D$ of size $m$. If $m > \frac{c}{\epsilon^2}(d\log \frac{d}{\epsilon}+\log\frac{1}{\delta})$, then  with probability $1-\delta$ over the choice of $S$, we have that $\forall h \in H$
$$\bigg|\frac{|h\cap S|}{|S|} - P(h)\bigg| < \epsilon$$
\end{theorem}

\begin{theorem}[Fundamental theorem of learning \cite{blumer1989learnability}] 
\label{thm:uniformConvergence}
Here, we state the theorem as in the book \cite{shalev2014understanding}. Let $H$ be a class of functions $h:\mc X \rightarrow \{0, 1\}$ of finite VC-Dimension, that is $\vcdim(H) = d < \infty$. Let $D$ be a probability distribution over $X$ and $h^*$ be some unknown target function. Given $\epsilon, \delta \in (0, 1)$. Let $err_D$ be the $\{0, 1\}$-loss function $err: H \rightarrow [0, 1]$. That is $err_D(h) = \underset{x \in D}{\mb P}[h(x) \neq h^*(x)]$. Sample a set $S = \{(x_1, y_1), \ldots, (x_m, y_m)\}$ according to the distribution $D$. Define $err_S(h) = \sum_{i=1}^{m} \frac{\mb 1_{[h(x_i) \neq h^*(x_i)]}}{m}$. If $m \ge a\frac{d + \log (1/\delta)}{\epsilon^2}$, then with probability atleast $1-\delta$ over the choice of $S$, we have that for all $h \in H$
$$|err_D(h) - err_S(h)| \le \epsilon$$
where $a$ is an absolute global constant. 
\end{theorem}

\begin{theorem}[Concentration inequality for sum of geometric random variables \cite{brown2011wasted}]
\label{thm:geometricRV}
Let $X = X_1 + \ldots + X_n$ be $n$ geometrically distributed random variables such that $\mb E[X_i] = \mu$. Then 
$$\mb P[X > (1+\nu)n\mu] \le \exp\bigg(\frac{-\nu^2\mu n}{2(1+\nu)}\bigg)$$
\end{theorem} 