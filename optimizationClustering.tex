\ifdefined\COMPLETE
\else
\documentclass[12pt]{article}

\usepackage{mathtools,amsmath,amssymb,amstext,amsthm,tikz,thmtools}
\usepackage[inline]{enumitem}
\usepackage{algorithm2e}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\newcommand{\mc}{\mathcal}
\newcommand{\mb}{\mathbf}
\newcommand{\tr}{\text{Tr}}
\newcommand{\diag}{\text{diag}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\title{Provably noise-robust $k$-means clustering}

\author{\normalsize
{Shrinu Kushagra} {\textnormal {,}} {Yaoliang Yu} {\textnormal {and}} {Shai Ben-David} \\
\normalsize David R. Cheriton School of Computer Science \\
\normalsize University of Waterloo,\\
\normalsize Waterloo, Ontario, Canada\\
\normalsize \{skushagr,yaoliang.yu,shai\}@uwaterloo.ca \\
}

\begin{document}
\fi

Clustering aims to group similar data instances together while separating dissimilar ones. However, often many datasets have, on top of cohesive groups, a subset of ``unstructured'' points as well. In such cases, the goal is to detect the structure while simultaneously separating the unstructured data points. Clustering algorithms that achieve this goal are said to be {\em robust} to noise.

The most common approach to clustering views it as an optimization problem. The idea is to associate a cost (or objective) with each possible partition (into $k$ subsets) of the input dataset and then try to find a partition which has minimum cost. Examples of common objective functions include $k$-means and $k$-median cost functions. However, it is easy to see that even the presence of one outlier can cause significant damage to the centers outputted by these methods.

In this chapter, we propose a generic method of regularization that can transform any clustering objective which outputs $k$ clusters to one that outputs $k+1$ clusters. The algorithm is now allowed to `discard' points into an extra `garbage' or noise cluster by paying a constant regularization penalty. The intuition is that allowing the clustering algorithm to discard a few points should make it easier to detect the structure in the remaining non-noisy points. However, we prove that finding the optimal solution to the regularized objective is NP-hard \footnote{Throughout this chapter, we consider the standard and regularized versions of the $k$-means objective}. 
%Similar to regularisation, {\em noise prototypes} (points which are equidistant to all other points) were considered by \cite{dave1993robust}. However, that idea was used only in the limited context of Lloydâ€™s algorithm and without any theoretical or noise robustness guarantees.

From a theoretical perspective, one common approach to dealing with such hardness results is the following. (i) Consider a convex relaxation of the original objective function (which can be efficiently solved using standard techniques). (ii) Prove that under certain data niceness conditions the solution obtained by solving the relaxed objective function coincides with the optimal solution of the original objective function. For example, \cite{peng2007approximating} used this strategy to design an algorithm based on the sdp-relaxation of the $k$-means objective function. \cite{awasthi2015relax} proved that under the `stochastic ball assumption', the solution of the sdp-based approach is indeed the optimal $k$-means clustering. 

One advantage of studying convex relaxations of clustering objectives over other clustering heuristics (like the Llyod's algorithm \cite{lloyd1982least}) is that we can verify when the output solution is optimal. There was been lot of work in analyzing clustering algorithms under distributional and clusterability assumptions (see \cite{balcan2012clustering}, \cite{awasthi2012center}, \cite{kalai2010efficiently}, \cite{achlioptas2005spectral}, \\\cite{sanjeev2001learning} and \cite{hsu2013learning} and the references therein). However, often the algorithms are designed keeping the input conditions in mind. Convex relaxations have an advantage that they are not specifically tailored to a particular distribution. In this chapter, we use this approach (sdp relaxation of the regularized $k$-means objective) to design an efficient and noise-robust clustering algorithm.

Our framework is the following. We are given an input dataset $X$ made of two components. The first is the clusterable subset $I$ which satisfies a niceness property. Namely, $I$ is the union of $k$ unit balls $B_i$ each separated by a distance of at least $\delta$. The second is the unstructured or noise component $N$. Note that the clustering algorithm only sees $X$ and is not aware of $I$ or $N$. The goal is to design an efficient clustering algorithm $\mc A$ such that the output of the algorithm $\mc A(X)$ when restricted to $I$, is able to detect and recover the structure of $I$ (namely, the balls $B_i$). In this chapter, we consider two choices for $\mc A$. The first based on SDP relaxation of the regularized $k$-means objective and the second based on the LP relaxation. For the noiseless case, \cite{awasthi2015relax} showed that for $\delta > 2\sqrt{2}(1+1/\sqrt d)$ (where $d$ is the dimension of the euclidean space) the algorithm based on SDP-relaxation of the standard $k$-means objective recovers (with high probability) the structure of $I$ if the balls $B_i$ are generated by an isotropic distribution (stochastic ball model \cite{iguchi2015tightness}, \cite{iguchi2017probably}, \cite{awasthi2015relax} and \cite{nellore2015recovery}).

In this chapter, we improve over previous results and give success guarantees in the regime $\delta > 2(1+\sqrt{k/d})$ which is near-optimal for large $d$. In the presence of noise, we prove that the algorithm based on the SDP-relaxation of the regularized objective recovers the clustering of $I$ for $\delta > 2(1+\sqrt{\zeta + k/d})$. Here $\zeta$ is a term which depends on the ratio of number of noisy points and the number of points in the smallest cluster. We obtain similar results for the LP based algorithm as well. However, in that the case both separation requirement and the restrictions on noise are stronger.  

We also conduct simulation studies where we examine the effect of $\lambda$, the number of noisy points $m$, the separation $\delta$ and other parameters on the performance of our regularised SDP-based algorithm. We also perform experiments on the MNIST dataset. We observed that the regularised version performed better than $k$-means++ when the dataset had noisy points. In the absence of noise, the performance of both these algorithms were similar. 

\section{Related Work}
\begin{table}
\centering
\caption{Known results for clustering under stochastic ball model. The columns show the separation under which recovery guarantees hold. All these results are for the noise-less case}

\label{table:stochasticBall}
\setlength{\tabcolsep}{0.5em} 
{\renewcommand{\arraystretch}{1.2}%
\begin{tabular}{llc}
\\
 & Separation condition & Reference\\
 \hline
$k$-median LP & $\delta > 3.75$ & Thm. 1 in \cite{nellore2015recovery}\\
 & $\delta > 2$ & Thm. 7 in \cite{awasthi2015relax}\\
\hline
$k$-means LP & $\delta > 4$ & Thm. 9 in \cite{nellore2015recovery}\\
\hline
$k$-means SDP & $\delta > 2\sqrt2 (1 + \frac{1}{\sqrt d})$ & Thm. 11 in \cite{awasthi2015relax}\\
 & $\delta > 2 + \frac{k^2}{d}cond(\gamma)$ & Thm. 2 in \cite{iguchi2015tightness}\\
 & $\delta > 2 + \frac{k^2}{d}$ & Thm. 9 in \cite{iguchi2017probably}\\
 & $\delta > 2 + \sqrt{\frac{k}{d}}$ & Thm. \ref {thm:SDPGeneral}\\
\hline


\label{table:alphacenter}
\end{tabular}
}
\end{table}
\cite{ben2014clustering} also studied the problem of robustifying a clustering objective function. They gave a generic procedure that can transform any clustering objective with metric $d$ to an objective function with metric $d'$, where $d'$ is a truncated version of the original metric. More specifically, $d'(x, y) = \min (\lambda,   d(x,y)).$ Then they show that if there exists an algorithm which can solve the optimization problem under the truncated metric then such an algorithm is robust to noise. \cite{georgogiannis2016robust}  also studies breakdown properties of the truncated objective function, that is the number of outliers which can cause significant change in the estimates of any one of the centers. However, both these papers do not make any comment about how one would solve the optimization problem for both the standard or the truncated versions. In this work, we propose an efficient algorithm based on convex relaxation of our regularized objective function and give robustness guarantees for the same. 

The problem of recovering the underlying cluster structure in the presence and absence of noise has been studied before both in distribution-free and distribution-based settings. In the distribution-free setting, the goal is to prove that if the data has some structure (is {\em clusterable}) then the (proposed or existing) clustering algorithm recovers that structure in the presence or absence of noise. Such works, make no assumption on the distribution that generated the data. Different works define different notions of `clusterable' data. In the current work, the separation requirement on the clusters was global. That is, the each cluster was separated by at least $\delta$ times the maximum radius amongst all the clusters. Another popular notion of clusterability is $\alpha$-center proximity \cite{awasthi2012center} which requires that two clusters be separated relative to their radii. \cite{balcan2012clustering} consider a dataset which has $\alpha$-center proximity except for an $\epsilon$-fraction of points. They propose an efficient algorithm which provides an $1+O(\epsilon)$ approximation to the optimal $k$-median solution for $\alpha > 2 + \sqrt{7}$.

In this work, we assume that the input satisfies the stochastic ball assumption. This model was considered by \cite{nellore2015recovery} in the context of LP relaxation of the $k$-median objective function. They showed that if the separation between the clusters $\delta > 3.75$, then the lp relaxation finds the desired clustering. This bound was later improved to $\delta > 2$ in \cite{awasthi2015relax}. They also considered the LP relaxation of $k$-means objective and gave recovery guarantees when $\delta > 4$. Along the same lines, \cite{awasthi2015relax} considered the sdp relaxation of the $k$-means objective. They showed similar recovery guarantees when $\delta > 2\sqrt 2 (1 + \sqrt d)$. This bound was later improved to $\delta > 2 + k^2/d$ by \cite{iguchi2017probably}. Table \ref{table:stochasticBall} contains a summary of the various results under the stochastic ball model. These results however apply only to the noiseless case. In this chapter, we consider the same framework but prove recovery guarantees even in the presence of a small fraction of noisy points. 

Another line of work, which has some similarities to the framework in this chapter is estimating the parameters of a distribution in the presence of noise. \cite{lai2016agnostic} considered the problem of mean estimation of a gaussian when $\epsilon$ fraction of the points are corrupted by noise and propose an efficient algorithm that can estimate it with almost linear samples. \cite{diakonikolas2016robust} acheive an error of $O(\epsilon\sqrt{\log \epsilon})$ for mean estimation which was later improved to $O(\sqrt\epsilon)$ \cite{diakonikolas2017being}, \cite{steinhardt2018resilience}. \cite{charikar2017learning} and \cite{diakonikolas2016robust} considered the problem of recovering a mixture of $k$ gaussians with few adversaries. \cite{charikar2017learning} show that it is possible to recover the means when the separation between the means is $\tilde \Omega(\sigma \sqrt{k})$ where $\sigma^2$ is an upper bound on the variance of the $k$ gaussians. \cite{diakonikolas2016robust} output a distribution which is close to the target distribution in total variation distance. However, their run-time is exponential in $k$. In this chapter our focus is on understanding the robustness properties of convex-relaxation type algorithms (specifically those based on relaxation of our regularized objective). Hence, these results our orthogonal to the current discussion.  

%Different works on clustering have also made different assumptions on the type of noisy points. The most common is to assume that the noise is adversarial but the number of adversaries is not too large. For example,  and  provide bounds for clustering in the presence of noise as long as the number of adversaries is constant-factor smaller compared to the size of the smallest cluster. \cite{kushagra2016finding} considered noise which is structureless, that is the noisy points do not form dense large subsets. Another field of work is to address noisy part of the data as being generated by some uniform random noise or gaussian perturbations \cite{cuesta1997trimmed}, \cite{garcia2008general} and \cite{dave1993robust}. Some works examine the robustness of different algorithms when the number of clusters is the same for the original data and the data with added noise. They show that in this setting the traditional algorithms are provably not noise-robust \cite{hennig2008dissolution} and \cite{ackerman2013clustering}. 

\cite{peng2007approximating} formulated the $k$-means objective as a 0-1 SDP and proved equivalence between the two formulations. They then relaxed the 0-1 SDP to a standard SDP. In this work, we use a similar proof and relaxation technique but for the regularised $k$-means objective. 

\section{Preliminaries and definition}
\label{sec:pre}

Let $(\mb M, d)$ be a metric space. Given a finite set $\mc X \subset \mb M$, a $k$-clustering $\mc C$ of $\mc X$ partitions the set into $k$ disjoint subsets $\mc C = \{C_1, \ldots, C_k\}$. An objective-based clustering algorithm associates a cost with each possible partition of $\mc X$ and then tries to find the clustering with minimum cost. Throughout this section, $f$ denotes a function on the nonnegative reals.

\begin{definition}[$(k, f)$-objective algorithm] Given $\mc X\subset \mb M$ and a distance function $d$, a $(k, f)$-objective based algorithm $\mc A$ tries to find centers $\mu_1, \ldots, \mu_k \in \mb M$ so as to minimize the following function
\begin{align}
\label{eqn:kfObjectiveAlg}
\text{Cost}(\mu_1, \ldots, \mu_k) = \sum_{x\in \mc X} f(d(x, \mu(x))), \quad \mu(x) = \argmin_{\mu \in \{\mu_1, \ldots, \mu_k\}} d(x, \mu).
\end{align}
\end{definition}
Note that algorithm $\mc A$ may not find the optimal solution because for many common functions $f$, solving the optimization is NP-hard. Thus, heuristics are used that can get stuck at a local minimum. For example, when $f(x) = x^2$, the above definition corresponds to the $k$-means objective, and the Lloyd's algorithm that is used to solve this objective can get stuck at a local minima. 

\begin{definition}[$(k, f)$-$\lambda$-regularised objective algorithm] Given $\mc X\subset \mb M$ and a distance function $d$, a $(k, f)$-$\lambda$-regularised objective based algorithm $\mc A'$ tries to find centers $\mu_1, \ldots, \mu_k \in \mb M$ and set $\mc I\subseteq \mc X$ so as to minimize the following function
\begin{align}
\label{eqn:kfObjectiveAlg}
\text{Cost}(\mu_1, \ldots, \mu_k, \mc I) = \sum_{x\in \mc I} f(d(x, \mu(x))) + \lambda |\mc X \setminus \mc I|,
\end{align}
where $\mu(x) = \argmin_{\mu_i} d(x, \mu_i).$
\end{definition}

The regularised objective allows discarding certain points into a ``garbage'' cluster at the expense of paying a constant penalty. The intuition is that this will help the algorithm better detect the structure of the remaining points. We will see in \S \ref{section:hardness} that minimizing this objective function is NP-hard for all $k \ge 1$.

\subsection{Robustification paradigms}
\begin{definition}[$\lambda$-Regularised Paradigm] The $\lambda$-regularised paradigm is a robustification paradigm which takes as input a $(k, f)$-objective algorithm $\mc A$ and returns a $(k, f)$-$\lambda$-regularised objective algorithm $\mc A'$.  
\end{definition}

In this work, we focus on robustification of the $k$-means objective. Hence, it is useful to define the regularised $k$-means objective as we will refer to it many times in the remainder of the chapter. 

\subsection{Robustness measure}
Given two clusterings $\mc C$ and $\mc C'$ of the same set $\mc X$, we define the distance between them, $\Delta(\mc C, \mc C')$, as the fraction of pairs of points which are clustered differently in $\mc C$ than in $\mc C'$. 
Given $\mc I \subseteq \mc X$, $\mc C|\mc I$ denotes the restriction of the clustering $\mc C$ to the set $\mc I$.

\begin{definition}[$\gamma$-robust \cite{ben2014clustering}]
Given $\mc X\subset \mb M$ and clustering algorithm $\mc A$, let $\mc A'$ be its robustified version obtained using any robustification paradigm. Given $\mc I \subseteq \mc X$, we say that $\mc I$ is $\gamma$-robust w.r.t $\mc X \setminus \mc I$ and $\mc A'$ if 
\begin{align}
\Delta(\mc A'(\mc X)|\mc I, \mc A(\mc I)) \le \gamma
\end{align}
\end{definition}
This measure tries to quantify the difference in the clustering of the set $\mc I$ after the addition of `noisy' points $\mc X \setminus \mc I$. If $\mc A'$ is indeed robust to noisy points, then the clusterings should be similar. 

\subsection{Regularised $k$-means objective}
Given a finite set $\mc X \subset \mb R^{d}$ and an integer $k$, the regularised $k$-means objective aims to partition the data into $k+1$ clusters $\mc C = \{C_1, \ldots, C_{k}, C_{k+1}\}$ so as to solve 
% Changed to put min in objective; seems clearer to me -Nicole
\begin{align}
	\min_{C_1,\ldots, C_{k+1} \atop c_1, \ldots, c_k} \sum_{i=1}^k \sum_{x \in C_i} \|x-c_i\|_2^2 + \lambda |C_{k+1}|.
	\label{eqn:modifiedkmeans}
\end{align}

Note that the first term of the objective depends on the $l_2$ norm, while the second term depends only on the cardinality of the ``noise'' cluster. In order to make our objective function invariant to scaling, the regularization constant $\lambda$ is added to the cost function. 

Let $m(\mc X) := \min_{x\ne y \in \mc X} \|x-y\|_2^2$ and $N = |\mc X|$. Then, it is easy to see when $\lambda \leq \frac{m(\mc X)}{2}$, then \eqref{eqn:modifiedkmeans} admits a trivial solution: each cluster $C_i$ for $i \leq k$ has exactly one point and all the remaining points are in $C_{k+1}$, leading to an objective $\lambda (N-k)$. Indeed, for any other clustering with $|C_i| = n_i$ its objective is at least $\sum_{i=1}^k (n_i-1) m(\mc X)/2 + \lambda n_{k+1} = (n-n_{k+1} - k) m(\mc X)/2 + \lambda n_{k+1}$, where we have used the simple fact:
\begin{align}
\forall C \subset \mc X, ~~
\min_{c} \sum_{x \in C} \|x - c\|_2^2 ~=~ \frac{1}{2|C|}\sum_{x, y \in C} \|x - y\|_2^2.
\end{align}
Comparing the objectives we see the solution is indeed trivial when $\lambda \leq m(\mc X) /2$. Surprisingly, for the interesting case when $\lambda > m(\mc X) /2$, the problem suddenly becomes NP-hard, as we prove below.





\section{Hardness of regularised $k$-means}
\label{section:hardness}

In this section, we present hardness results for the regularised $k$-means objective. 

We consider the interesting case of the regularized $1$-means problem. It is well-known that $1$-means can be solved in linear time \cite{Bellman73}. However, we will show by reducing an instance of the MAX-CLIQUE problem to the regularised $1$-means problem is NP-hard. We give a proof sketch here. The technical details can be found in the appendix.

\begin{restatable}{theorem}{hardForkone}
\label{theorem:hardFork1}
For all $\lambda$ there exists a clustering instance $ X \subset \mb R^p$ such that $\lambda > \frac{m(X)}{2}$ but finding the optimal solution to the regularized $1$-means objective is NP-hard where recall that $m(X) := \min_{x\ne y \in X} \|x-y\|^2$.
\end{restatable}

\begin{proof}[Proof sketch]
The proof has two parts. We first show that for fixed $\lambda$ the problem is NP-hard. The proof works by reducing an instance of MAX-CLIQUE to the regularised $1$-mean instance. The idea is to define the distance between any pair of vertices as $1$ if there exists an edge between them. If not, then define the distance as $1 + \Delta$ for a suitably chosen $\Delta$. This construction guarentees that the problem is NP-hard for at least one $\lambda > \frac{m(\mc X)}{2}$. Our result then follows using a scaling argument.
\end{proof}

Note that the same argument (as in the appendix) infact works for $k \ge 2$ where we would reduce from the MAX-$k$-CLIQUE problem (cover maximum vertices by $k$ cliques). We leave this proof as a simple exercise for the reader. Thus, we show that regularised $k$-means is hard for all $k \ge 1$. 

\section{The regularised $k$-means SDP-based algorithm}
\label{section:heuristic}
In the previous section, we showed that the regularised $k$-means objective is NP-hard to optimize. Hence, we cannot hope to solve the problem exactly unless $P=NP$. In this section, we develop an algorithm based on semi-definite programming relaxation of the regularised objective. 

\cite{peng2007approximating} developed an algorithm $\mc A$ (Alg. \ref{alg:heuristicSDP} with $\lambda = \infty$) which tries to minimize the $k$-means objective. They obtained a convex relaxation of the $k$-means objective and solved it polynomially using standard solvers. In this section, we use the same technique to obtain and efficiently solve the convex relaxation of the regularised $k$-means objective. Our algorithm $\mc A'$ (Alg. \ref{alg:heuristicSDP}) is the robustified version of $\mc A$ using the $\lambda$-regularised paradigm. In \S \ref{subsection:sdpAlg}, we give the details of how we transform the regularised objective into an SDP. \S \ref{subsection:sdpRobust} has our main results where we give robustness guarentees for $\mc A'$.    

\RestyleAlgo{ruled}
\SetAlgoNoLine
\LinesNumbered
\begin{algorithm}[t]
\caption{SDP-based regularised $k$-means algorithm}
\label{alg:heuristicSDP}
	\Indp\KwIn{ $\mc{X} \subset R^d$, $k$, and hyperparameter $\lambda$.}
	\KwOut{$\mc C' := \{C_1, \ldots, C_k, C_{k+1}$\}.}
	
	Compute the matrix $D_{ij} = \|x_i-x_j\|^2_2$.\\
	Solve the SDP (Eqn. \ref{eqn:regularizedSDP}) using any standard SDP solver and obtain matrix $Z$ and vector $y$.\\
	Use the rounding procedure (Alg. \ref{alg:roundSDP}) to obtain the partition $\mc C'$. 
\end{algorithm}

 
\subsection{The SDP-based algorithm}
\label{subsection:sdpAlg}
The SDP relaxation of the regularised $k$-means objective is obtained in two steps. Using similar technique to that of \cite{peng2007approximating}, we translate Eqn. \ref{eqn:modifiedkmeans} into a 0-1 SDP (Eqn. \ref{eqn:regularizedSDP}). We then prove that solving the 0-1 SDP exactly is equivalent to solving the regularised $k$-means problem exactly. Then, we relax some of the constraints of the 0-1 SDP to obtain a tractable SDP which we then solve using standard solvers. We then describe the rounding procedure which uses the solution of the SDP to construct a clustering of the original dataset. 

\begin{equation*}
	\begin{split}
	\textbf{0-1}\\
	\textbf{SDP}
  \end{split}
	\begin{cases}
		\min_{Z, y} \enspace &\tr(DZ) + \lambda \langle \mb 1, y\rangle\\
		\text{s.t. } \enspace &\tr(Z) = k\\
		& Z\cdot \mb 1 + y = \mb 1\\	
		&Z\ge 0, Z^2 = Z,Z^T = Z \\
		& y \in \{0, 1\}^n
	\end{cases}
	\xrightarrow{\text{relaxed}} \textbf{ SDP } 
	\begin{cases}
		\min_{Z, y} \enspace &\tr(DZ) + \lambda \langle \mb 1, y\rangle\\
        \text{s.t. } \enspace &\tr(Z) = k\\
        \\
		& \Big(\frac{Z+Z^T}{2}\Big)\cdot \mb 1 + y = \mb 1\\		
		&Z \ge 0, y \ge 0, Z \succeq 0 \numberthis\label{eqn:regularizedSDP}
	\end{cases}
\end{equation*}

\begin{restatable}{theorem}{modifiedkmeans}
\label{thm:modifiedkmeans}
Finding a solution to the 0-1 SDP (\ref{eqn:regularizedSDP}) is equivalent to finding a solution to the regularised $k$-means objective (\ref{eqn:modifiedkmeans}). 
\end{restatable}

Equation \ref{eqn:regularizedSDP} shows our 0-1 SDP formulation. The optimization is NP-hard as it is equivalent to the regularised $k$-means objective. Hence, we consider a convex relaxation of the same. First, we replace $Z^2 = Z$ with $Z \succeq 0$. In addition, we relax $y \in \{0, 1\}^n$ to $y \geq 0$, as the constraint $y\leq 1$ is redundant. Using these relaxations, we obtain the SDP formulation for our objective function.

We solve the SDP using standard solvers \cite{yang2015sdpnal+} thereby obtaining $Z, y$. 
The proof of Thm. \ref{thm:modifiedkmeans} showed that the optimal solution of 0-1 SDP is of the following form. $Z$ is a $n\times n$ block diagonal matrix of the form $\diag(Z_{I_1}, \ldots, Z_{I_k}, 0)$, where $n = |\mc X|$ and $Z_{I_i} = \frac{1}{|C_i|}\mb 1 \enspace\mb 1^T$. Thus, given $Z$, we can extract the set of cluster centers $C = ZX$ which is an $n \times d$ matrix. Each row $C_i$ contains the cluster center to which data point $x_i$ belongs. For the points $x_j$ assigned to the noise cluster, the corresponding row $C_j$ is zero and $y_j = 1$. The SDP solver does not always return the optimal solution, as the relaxation is not exact. However, we expect that it returns a near-optimal solution. Hence, given $Z$ and $y$ returned by the solver, we use Alg. \ref{alg:roundSDP} to extract a clustering of our original dataset. The $threshold$ parameter indicates our confidence that a given point is noise. In our experiments, we have used a threshold of $0.5$. 

\RestyleAlgo{ruled}
\SetAlgoNoLine
\LinesNumbered
\begin{algorithm}[t]
\caption{Regularised $k$-means rounding procedure}
\label{alg:roundSDP}
	\Indp\KwIn{ $Z \subset \mathbb{R}^{n\times n}$, $y \subset \mathbb{R}^{n}$, $\mc{X}$, and $threshold \in [0,1]$.}
	\KwOut{$\mc C'$.}
	
	If $y_i > threshold$ then
	\begin{itemize}[nolistsep] 
		\item[] Delete $z_i$ and $z_i^T$ from $Z$. Put $x_i$ in $C_{k+1}$.
		\item[] Delete $x_i$ from $X$.
	\end{itemize}
	$k$-cluster the columns of $X^TZ$ to obtain clusters $C_1, \ldots, C_k$.\\
	Output $\mc C' = \{C_1, \ldots, C_k, C_{k+1}\}$.
\end{algorithm}

\subsection{Robustness guarantees}
\label{subsection:sdpRobust}

Assume that we are given a set $\mc I$ of $k$ well-separated balls in $\mb R^d$. That is, $\mc I := \cup_{i=1}^k B_i$ where each $B_i$ is a ball of radius at most one and centered at $\mu_i$ such that $\|\mu_i - \mu_j\| \ge \delta$. On top of this structure, points are added from the set $\mc N$. Let $\mc A$ and $\mc A'$ be the SDP based standard and regularised $k$-means algorithm respectively (as defined in the begining of \S \ref{section:heuristic}). We will show that $\mc I$ is $0$-robust w.r.t $\mc A'$ and $\mc N$ under certain conditions on $\delta$ and mildness properties of the set $\mc N$. To show this, we need to compare the clusterings $\mc A(\mc I)$ and $\mc A'(\mc X)|_{\mc I}$. We first prove recovery guarentees for $\mc A$ in the absense of noisy points.

\begin{theorem}
\label{thm:SDPGeneral}
Let $\mc P$ denote the isotropic distribution on the unit ball centered at origin in $\mb R^d$. Given points $\mu_1, \ldots, \mu_k$ such that $\|\mu_i - \mu_j\| > \delta  > 2$. Let $\mc P_i$ be the measure $\mc P$ translated with respect to the center $\mu_i$. Let each $B_i$ is drawn i.i.d w.r.t the distribution $\mc P_i$. 

Given a clustering instance $\mc I \subset \mb R^{N\times d}$ and $k$ where $\mc I := \cup_{i=1}^k B_i$. Define $ n := \min_{i\in[k]} |B_i|$ and $\rho = \frac{N}{nk}$. If
$$\delta > 1 + \sqrt{1+\frac{2\theta\rho k}{d}\Big(1+\frac{1}{\log N}\Big)^2}$$  
where $\theta = \mb E[\|x_{pi}-c_p\|^2] < 1$, then there exists a constant $c > 0$ such that with probability at least $1 - 2d\exp(\frac{-cN\theta}{d\log^2N})$ the $k$-means SDP (Alg. \ref{alg:heuristicSDP} with $\lambda = \infty$) finds the intended cluster solution  $\mc C^* = \{B_1, \ldots, B_k\}$.
\end{theorem}

Thm. \ref{thm:SDPGeneral} improves the result of Thm. 11 in \cite{awasthi2015relax}. Under the stochastic ball assumption, they showed that the $k$-means SDP finds the intended solution for $\delta > 2\sqrt{2}(1+\frac{1}{\sqrt{d}})$. For $k \ll d$, which is the case in many situations our bounds are optimal in terms of the separation requirement of the clusters.  \cite{iguchi2015tightness} obtained similar results but for $\delta > 2 + \frac{k^2}{d}cond(\mc I)$. However, the condition number (ratio of maximum distance between any two centers to the minimum distance between any two centers) can be arbitrarily large. \cite{iguchi2017probably} improve the separation requirement to $\delta > 2 + \frac{k^2}{d}$. Asymptotically, as $d$ goes to $\infty$ their bound matches our result. We also have a better dependence on the number of clusters $k$ while they have a better dependence on the dimension $d$. 

Next, we analyse the recovery guarentees for $\mc A'$ in the presence of noisy points $\mc N$. We decompose the noisy points into two disjoint sets $\mc N_1$ and $\mc N_2$. The set $\mc N_2$ consists of all the points which are far from any of the points in $\mc I$. The set $\mc N_1$ consists of points which are close to at least one of the clusters. We also require that any point in $\mc N_1$ has an $\alpha$-margin w.r.t to the centers of the balls $B_1, \ldots, B_k$. That is the difference of the distance between any point in $\mc N_1$ to a cluster center is at least $\alpha$. Now, we will show that if $\mc N$ has the aforementioned properties then $\mc I$ is robust w.r.t the regularised SDP algorithm $\mc A'$.

\begin{theorem}
\label{thm:regularizedSDPGeneral}
Let $\mc P$ denote the isotropic distribution on the unit ball centered at origin in $\mb R^d$. Given centers $\mu_1, \ldots, \mu_k$ such that $\|\mu_i - \mu_j\| > \delta > 2$. Let $\mc P_i$ be the measure $\mc P$ translated with respect to the center $\mu_i$. Let $B_i$ is drawn i.i.d w.r.t the distribution $\mc P_i$. 

Given a clustering instance $\mc X \subset \mb R^{N \times d}$ and $k$. Let $\mc X : = \mc I \cup \mc N$ where $\mc I := \cup_{i=1}^k B_i$. Let $\mc N = \mc N_1 \cup N_2$ have the following properties. For all $n \in N_1$ and for all $i, j$, we have that $| \|n-\mu_i\|^2 - \|n-\mu_j\|^2| \ge \alpha$. For all $n \in \mc N_2$ and for all $x \in \mc I, \|n- x\| \ge \nu \ge \sqrt{(\delta-1)^2+1}$. Note that $\mc N_1 \cap \mc N_2 = \phi$. Let $n = \min_i |B_i|$ and $\epsilon = \frac{|\mc N_1|}{n}$ and $\rho = \frac{|I|}{nk}$. If  

\begin{itemize}
  \item $\delta > 2 + \sqrt{ O(\epsilon) + \frac{2\rho k\theta(1+1/\log(|I|))^2}{d}}$ 
  \item $\alpha \ge O(\epsilon)+ \frac{2\rho k\theta(1+1/\log|I|)^2}{d}$ 
  \item $\frac{|\mc N_2|}{n} \le \frac{\delta^2-2\delta-O(\epsilon)}{\lambda}$
\end{itemize}
then there exists a constant $c_2 > 0$ such that with probability at least $1 - 2d\exp(\frac{-c_2|I|\theta}{d\log^2|I|})$ the regularised $k$-means SDP (Alg. \ref{alg:heuristicSDP}) finds the intended cluster solution  $\mc C^* = \{C_1, \ldots, C_k, \mc N_2\}$ where $B_i \subseteq C_i$ when given $\mc X$ and $\delta^2+2\delta \ge \lambda \ge (\delta-1)^2 + 1$ as input.
\end{theorem}

The proof of both the Thms. \ref{thm:SDPGeneral} and \ref{thm:regularizedSDPGeneral} use the following ideas. We construct a dual for the SDP. We then show that when the conditions of our theorems are satisfied then there exists a feasible solution for the dual program. Moreover, the objective function value of primal and dual sdp program are the same. Hence, the solution found is indeed optimal. We use similar techniques as in the proof of Thm. 11 in \cite{awasthi2015relax}. However, our analysis is tighter which helps us to obtain better bounds. The details are in the appendix.

We have also developed a regularized version of the $k$-means LP based algorithm. The details and the robustness guarantees for the LP-based algorithm are in the appendix. 

\section{Experiments}
We ran several experiments to analyse the performance of our regularised $k$-means algorithm. The first set of experiments were simulations done on synthetic data. The second set of experiments were done on real world datasets like MNIST where we compared the performance of our algorithm against other popular clustering algorithms like $k$-means++. All our experiments were run on Matlab. We solved the SDP formulation using the Matlab SDPNAL+ package \cite{yang2015sdpnal+}. To run $k$-means++ we used the standard implementation of the algorithm available on Matlab.

\subsection{Simulation studies}
The goal of these sets of experiments was to understand the effect of different parameters on the performance of the regularised SDP algorithm. Given the number of clusters $k$, the separation between the clusters $\delta$, the dimension of the space $d$, the number of points in each cluster $n$ and the number of noisy points $m$. We generate a clustering instance $\mc X$ in $\mb R^d$ as follows. We first pick $k$ seed points $\mu_1, \ldots, \mu_k$ such that each of these points are separated by at least $\delta$. Next we generate $n$ points in the unit ball centered at each of the $\mu_i's$. Finally we add $m$ points uniformly at random. 

We analyse the performance of the regularised SDP algorithm as the parameters change. The most crucial amongst them is the separation between the clusters $\delta$, the regularization constant $\lambda$ and the dimension $d$. Fig. \ref{figure:simulation} shows the heatmap under different parameter values. For each setting of the parameters, we generated 50 random clustering instances. We then calculated the fraction of times the regularised sdp was able to recover the true clustering of the data. If the fraction is close to one, then its color on the plot is light. Darker colors represent values close to zero.

We see an interesting transition for $\lambda$. When $\lambda$ is `too small' then the probability of recovering the true clustering is also low. As $\lambda$ increases the probability of success goes up which are represented by the light colors. However, if we increase $\lambda$ to a very high value then the success probability again goes down. This shows that there is a `right' range of $\lambda$ as was also predicted by our theoretical analysis. 

Another parameter of interest is the dimension of the space $d$. Note that from our theoretical analysis, we know that both the probability of success and the separation depend on $d$. Fig. \ref{figure:simulation} shows that for very low dimension, the regularised sdp fails to perfectly recover the underlying clustering. However, as the dimension grows so does the probability of success. For these two simulations, we fixed the number of points per cluster $n = 30$, $k = 8$ and the number of noisy points $m = 30$. We have similar plots for $(\delta, n)$ and $(\delta, k)$ and $(\delta, m)$ and $(n, m)$. These plots very mostly light colored as long as the number of noisy points was not too large ($\frac{m}{n} \le 5$). Hence, due to space constraints, we have included them only in the appendix.
   
\begin{figure}[t]
  \centering
  \includegraphics[width=0.4\textwidth]{figures/optimizationClustering/deltaLambda.png}
  \includegraphics[width=0.4\textwidth]{figures/optimizationClustering/deltaD.png}
  \caption{Heatmap showing the probability of success of the $k$-means regularised sdp algorithm. Lighter color indicates probability closer to one while darker indicates probability closer to zero.}  
  \label{figure:simulation}
\end{figure}

\subsection{Results on MNIST dataset}
We compare our regularised SDP algorithm against $k$-means++ on the MNIST dataset. MNIST is a dataset of images of handwritten digits from zero to nine. It contains 60,000 training images and 10,000 test images. We choose $k = 4$ different classes and randomly sample a total of $N = 1,000$ images from these classes.  We then run both our regularised SDP algorithm and the $k$-means++ algorithm on this dataset. We repeat this process for 10 different random samples of MNIST. We measure the performance of the two algorithms in terms of the precision and recall over the pairs of points in the same cluster. Given a clustering $\mc C$ and some target clustering $C^*$. Define the precision $p$ of $\mc C$ as the fraction of pairs that were in the same cluster according to $\mc C^*$ given that they were in the same cluster according to $\mc C$. The recall $r$ of $\mc C$ is the fraction of pairs that were in the same clustering according to $\mc C$ given that they were in the same cluster according to $\mc C^*$. We finally measure the $f_1$ score of the clustering $\mc C$ as the harmonic mean of its precision and recall. $f_1 = \frac{2pr}{p+r}$. 

Note that the regularised algorithm outputs $k+1$ clusters. Hence, to make a fair comparison, we finally assign each point in the noisy cluster ($C_{k+1}$) to one of the clusters $C_1, \ldots, C_k$ depending upon the distance of the point to the clusters. Another point is that the $f_1$ measures are sensitive to the choice of the $k$ digits or classes. For some choice of $k$ classes, the $f_1$ measures for both the algorithms are higher than compared to other classes. This shows that some classes are more difficult to cluster than other classes. Hence, we only report the difference in performance of the two algorithms. 

We report the performance on datasets with and without noisy points. The first is when there are no outliers or noisy points. In this case, the difference in the $f_1$ values was about $4.34\%$ in favor of $k$-means++. We then added noisy points to the dataset. In the first case, we added images from different datasets like EMNIST (images of handwritten letters). In this case, the difference was $2.54\%$ in favor of the regularised algorithm. In the second case, besides images from different datasets, we also added a few random noisy points to the MNIST dataset. In this case, the difference increased further to about $6.9\%$ in favor of the regularised algorithm. 

\section{Conclusion}
We introduced a regularisation paradigm which can transform any center-based clustering objective to one that is more robust to the addition of noisy points.  We proved that regularised objective is NP-hard for common cost functions like $k$-means. We then obtained regularised versions of an existing clustering algorithm based on convex (sdp) relaxation of the $k$-means cost. We then proved noise robustness guarentees for the regularised algorithm. The proof improved existing bounds (in terms of cluster separation) for sdp-based standard (non-regularised) $k$-means algorithm. Our experiments showed that regularised sdp-based $k$-means performed better than existing algorithms like $k$-means++ on MNIST especially in the presence of noisy points.

\ifdefined\COMPLETE
\else 
\end{document}
\fi