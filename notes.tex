\documentclass[12pt]{article}

\begin{document}

\section{Introduction}
\subsection*{Slide 2}
What are the different approaches to clustering?
\begin{itemize}
	\item Cost-based clustering\\
	Find minimum cost partition. $k$-means (minimize sum of squared distances), $k$-median (minimize sum of distances, centers from data), fuzzy $k$-means (aims to minimize the weighted sum of squared distances), $k$-medoids (Partitioning Around Medoids). \\
	All the algorithms are some variants of local search heuristics.
	\item Hierarchical clustering\\
	Start with every point in its own cluster. Merge till one cluster remains. Common ways to merge, single-linkage, max-linkage, avg-linkage.
	\item Spectral clustering\\
	Let $L = D^{-1/2} A D^{-1/2}$ (where $A$ is adjacency matrix). The graph can be constructed in various ways ($\epsilon$-neighbourhood, $k$-nearest neighbour, weighted fully connected graph). Find top $k$ eigenvalues of this matrix and cluster using $k$-means.\\
	There are relations between these and kernel PCA.
\end{itemize}
Some recent work on cost-based hierarchical clustering. The cost is based on trying to put similar points higher on the tree.\\

\noindent Different definitions of clustering?
\begin{itemize}
	\item Soft $k$-means: Membership is probabilistic and not hard.
\end{itemize}

\subsection*{Slide 4}
Computational complexity of clustering.
	\begin{itemize}
		\item $[$Dasgupta' 08$]$\\
		Reduction from NAE3SAT. Hard for $k=2$ and $d = n$.

		\item $[$Vattani' 09$]$\\
		Planar construction. Hard for $d=2$ and $k=n^{\epsilon}$.

		\item $[$Awasthi et. al '15$]$\\
		There exists a constant $\epsilon$ such that it is NP-Hard to approximate $k$-means within $(1+\epsilon)$. Reduction from  vertex cover on triangle free graphs which is NP-Hard to approximate within $1.36$ for $k = \Omega(n)$.

		\item $[$Megiddot and Supowit$]$\\
		The $k$-median problem as well as the $k$-center problem is NP-Hard. They even show that $k$-center is NP-Hard to approximate within a constant factor for $d = 2$ for $k =\Omega(n)$.

		\item $[$Arya et. al '01$]$\\
		The $k$-median problem has a local search based algorithm with approximation ratio $3+\epsilon$.\\
	This has been improved to $1+\sqrt{3}+\epsilon$.
	\end{itemize}

\subsection*{Slide 10-11}
Definition of structure or clusterability conditions.
	\begin{itemize}
		\item $\alpha$-center proximity\\
		$k$-means efficiently solvable under center-proximity for $\alpha > 2 + \sqrt{3}$. NP-Hard for $\alpha \le 3$.\\
		 $k$-median efficiently solvable under center-proximity for $\alpha > 1 + \sqrt{2}$. NP-Hard for $\alpha \le 2$
		 
		 \item $(\alpha, \epsilon)$-center proximity\\
		 Efficient $(1+\frac{8n}{m(C)}\epsilon)$-approximation to the $k$-median objective for $\alpha > 2 + \sqrt{7}$.
		 
		 \item $\epsilon$-separatedness\\
		 $Cost(OPT(k)) \le \epsilon^2 Cost(OPT(k-1))$. Variant of $k$-means finds a $(1+\epsilon)$-approximation with high probability in time $O(nk+k^3)$. \\
		 For $k=2$, algorithm succeeds with probability $1-\frac{20\epsilon^2}{\sqrt{(1-\epsilon^2)(1-101\epsilon^2)}}$. This implies average radius much smaller than separation.
		 $r_i < \frac{\epsilon}{\sqrt{1-\epsilon^2}} d(c_i, c_j)$.  
	\end{itemize}

\subsection*{Slide 14}
Incorporating domain knowledge into clustering.
	\begin{itemize}
		\item $[$Wagstaff et. al' 01$]$\\
		A set of constraints is provided to the algorithm in the beginning. COP $k$-means is a modified version of Llyod's algorithm where you assign points to closest center such that the constraints are satisfied.
		\item $[$Ashtiani, Ben-David `15$]$\\
		The user can cluster a small subset. Learn a model consistent with that clustering. Sample complexity of learning the model. The goal is to learn a mapping under which $k$-means clustering is close to the target clustering. Prove that an ERM approach has `small' sample complexity.
		\item $[$Balcan, Blum `08$]$\\
		The oracle responds with either split a cluster or merge two clusters. Given a class $C$ of clusters, recover the target clustering. A generic but inefficient procedure which makes $O(k^3 \log |C|)$ queries.
		\item $[$Awasthi, Balcan, Voevodski '17$]$\\
		Merge-split based algorithm to recover the target clustering. If the initial clustering has atmost $\delta$ over-clustering and under-clustering error (these quantities are upper bounded by $k^2$) then the algorithm can recover target by making atmost $\delta$ merge-split queries. \\
		For correlation clustering, if the initial clustering has error $\delta$ (where $\delta$ is the number edges which are incorrect), their algorithm recovers the target by making atmost $\delta$ queries. For correlation clustering $\delta = O(n)$. Using our method, we can recover the target using only linear queries and the user doesn't need to evaluate the whole clustering everytime.
	\end{itemize}
\subsection*{Slide 22}
The proof of positive result relies on two lemmas. The first lemma says that if $d(\mu, \mu') \le \epsilon$ and $\gamma > 1 + 2\epsilon$, then $d(x, \mu') < d(y, \mu')$. Hence, there is a separation even w.r.t to the center $\mu'$.

The second lemma uses a concentration inequality in high dimensions. Namely, if $n > c\frac{\log (1/\delta)}{\epsilon^2}$ then $\|\mu - \frac{1}{n}\sum X_i\|^2 \le R^2 \epsilon$ 

\subsection*{Slide 24}
The point of the proof is the following. Any clustering of $X$ which costs $\le L$ has the following property. Atleast $m$ of rows $R_i$ are be grouped in a $A$-type clustering and the rest are grouped in $B$-type clustering. The rows of $G_i$ are singletons. And the points in $Z_i$ are added to clusters which are good for it (increase the cost by $\frac{2}{\sqrt{3}}h$. Other clusterings have cost atleast $L + \frac{w}{3}$. 

If $S_i = (s_{i1}, s_{i2}, s_{i3})$ is included in the exact cover then the row $R_i$ should be grouped in a $A$-type clustering. Then $x'_{ij}$ location will force all other dash locations to be in $B$-type clusterings (as the up ones will have to go `up' and the down ones will have to go down). Since there are $3m$ variables they will have exactly $m$ rows in $A$-type clustering. 

\subsection*{Slide 29}
Designing a distance metric.
\begin{itemize}
	\item Character-based: Edit distance and its many variants.
	\item Token-based: Word (or token) representations. Word2vec or tf-idf measures.
	\item Phonetic-based: Soundex, NYSIIS. Replace similar sounding constants with numbers or letters.
	\item Numerical
\end{itemize}
Learning a distance metric.
\begin{itemize}
	\item Learning the weights of (distance functions across) different fields.
	\item Learning a classifier over pairs. (Can view as a $\{0, 1\}$ distance function.)
\end{itemize}

\subsection*{Slide 30}
\begin{itemize}
	\item $[$Bansal et al., 2004$]$\\
	Introduce the problem of correlation clustering (or min-disagree). Give a constant factor approximation to the problem. But the constant is huge. 
	\item $[$Demaine et al., 2006$]$\\
	Gave a $4\log_2 n$ approximation to the minimization problem. Gave a $O(\log_2 n)$ approximation to the weighted correlation clustering problem. 
\end{itemize}
\subsection*{Slide 31}
Four definitions and thoughts.

\subsection*{Slide 32}
Question about precision and recall.

\subsection*{Slide 36}
Hardness proof details.

\subsection*{Slide 40}
Hardness with query proof details.

\subsection*{Slide 55}
VC proof


\subsection*{Slide 57}
Weighted clustering

\end{document}























































