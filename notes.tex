\documentclass[12pt]{article}
\usepackage{amsmath}

\begin{document}

\section{Concepts}
\subsection{Spectral clustering}
Let $L = D^{-1/2} A D^{-1/2}$ (where $A$ is adjacency matrix). The graph can be constructed in various ways ($\epsilon$-neighbourhood, $k$-nearest neighbour, weighted fully connected graph). Find top $k$ eigenvalues of this matrix and cluster using $k$-means.\\
	There are relations between these and kernel PCA.
\subsection{Hardness}
\begin{itemize}
	\item \textbf{P}: Class of problems solvable in polynomial time.
	\item \textbf{NP}: Class of decision problems verifiable in polynomial time.
	\item \textbf{NP-Complete}: Class of problems in $NP$ such that every problem in NP reduces to it in polynomial time. 
	\item \textbf{NP-Hard}: Class of problems such that every problem in NP reduces to it in polynomial time. 
	\item \textbf{Strong NP-Complete (or NP-Hard)}: If it remains NP-Complete (or NP-Hard) so even when all of its numerical parameters are bounded by a polynomial in the length of the input.
	\item \textbf{PTAS}: Polynomial time approximation scheme. A problem admits a PTAS if there exists an algorithm which finds a $(1+\epsilon)$ approximation in run-time which is polynomial in $n$ for a fixed $\epsilon$. 
	\item \textbf{FPTAS}: Fully Polynomial time approximation scheme. Admits a PTAS with run-time polynomial in both $n$ and $\frac{1}{\epsilon}$. 
	\item \textbf{APX}: Polynomial time approximation (constant-factor) algorithm.
	\item \textbf{APX-Hard}: There is a PTAS reduction from every problem in APX to that problem.
	\item \textbf{Approximation preserving or PTAS-reduction}: A reduction from problem $A$ to $B$ such that an approximate solution (or a ptas solution) to $B$ can be used to extract an approximate (or ptas) solution for $A$.
\end{itemize}
If $P \neq NP$ then
\begin{itemize}
	\item \textbf{FPTAS} $\not\subseteq$ \textbf{PTAS} $\not\subseteq$ \textbf{APX}.
	\item \textbf{APX-Hard} problems do not have PTAS. 
	\item \textbf{NP-Hard} problems do not have polynomial algorithms. 
\end{itemize}

\subsection{Convex program}
Linear program can be solved in $O(n^{2.5})$.

\subsection{Learning mixtures of gaussians}

Given a distribution of mixture of $k$ gaussians in $R^d$. Estimate the parameters $\mu$ and $\sigma$ of these gaussians. Given a sample of size $n$ generated according to the distribution.

\begin{itemize}
	\item Sanjoy Dasgupta, {\em Learning mixtures of gaussians. [Das99]}\\
	Algorithm based on random projections\\
	Recovers the centers of the gaussians when the separation is $\sigma \sqrt d$.\\
	The number of samples required is $n = k^{O(1)}$.
	
	\item Sanjoy Dasgupta and Leonard Schulman, {\em A probabilistic analysis of em for mixtures of separated, spherical gaussians [DS07]} \\
	Analyze the performance of the EM algorithm when the data is generated by a mixture of $k$-gaussians.\\
	Recovers the centers of the gaussians when the separation is $\sigma d^{\frac{1}{4}}$.\\
	The number of samples depend on parameters of the distribution.
	
	\item Sanjeev Arora and Ravi Kannan, {\em Learning mixtures of separated nonspherical Gaussians [AK05]}\\
	Reduce the separation required to learn a mixture of gaussians to $d^{\frac{1}{4}}$ in the spherical case.\\
	They construct an algorithm.
	
	\item Santosh Vempala and Grant Wang, {\em A spectral algorithm for learning mixtures of distributions [VW02]}\\
	Propose a spectral algorithm for the problem.\\
	When the distribution is spherical, separation required is only $\|\mu_i - \mu_j\| \ge C \max \{\sigma_i, \sigma_j \} k^{\frac{1}{4}}\log^{\frac{1}{4}}d$
	
	\item Dimitris Achlioptas and Frank McSherry, {\em On spectral learning of mixtures of distributions [AM05]}\\
	Algorithm based on combining spectral projection with single-linkage.\\
	Recovers mixture of gaussians when separation is $4\sigma_i(k\log Mk + k^2)^{\frac{1}{2}}$ where $M >> k(d+ \log k)/w_{min}$
	
	\item Amit Kumar and Ravindran Kannan, {\em Clustering with spectral norm and the k-means algorithm [KK10]}\\
	Analyze the convergence of the Llyod's algorithm\\
	Show convergence when the separation is $k \times radius$. 
	\item Jon Feldman, Rocco A. Servedio, and Ryan O’Donnell, {\em PAC learning axis-aligned mixtures of gaussians with no separation assumption [FSO06]}\\
	PAC model for learning a mixture of gausssians.\\
	Given a class $C$ of probability distributions over $R^d$ and access to random data sampled from an unknown distribution $Z \in C$. Find $Z' \in C$ which with high confidence is $\epsilon$-close to $Z$ in KL-divergence.\\
	Give an algorithm which recover the mixture of gaussians and runs in time $ploy(n/\epsilon).\log(1/\delta)$.\\
	Polynomial in $n$ only if $k$ is a constant. 
	
	\item Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant, {\em Efficiently learning mixtures of two gaussians [KMV10]}\\
	In the mixtures of gaussian model, covariance matrix is the same.\\
	They consider GMM (Gaussian mixture model).\\
	Similar result as [FSO06].\\
	Their algorithm recovers the parameters such that variation distance between the estimated and true distributions is small.
	
	\item Ankur Moitra and Gregory Valiant, {\em Settling the polynomial learnability of mixtures of gaussians [MV10]}\\
	Extend the previous result to mixture of $k$ gaussians.
	
	\item Mikhail Belkin and Kaushik Sinha, {\em Polynomial learning of distribution families [BS10]}\\
	Prove that the parameters of a Gaussian mixture can be learned with precision $\epsilon$ and confidence $1−\delta$ using number of samples which are polynomial in $n$.\\
	Their results hold for arbitrary separation.\\
	The computational complexity is exponential in $k$.
	
	\item Daniel Hsu and Sham M. Kakade, {\em Learning mixtures of spherical Gaussians: moment methods and spectral decompositions [HK13]}\\
	If the  means  of  the  gaussians  are  linearly  independent,  their algorithm can learn the model in polynomial time and with polynomial number of samples.
	
	\item Joseph Anderson, Mikhail Belkin, Navin Goyal, Luis Rademacher, and James R. Voss, {\em The more, the merrier: the blessing of dimensionality for learning large gaussian mixtures [ABG + 14]}\\
	Learning gaussians under certain conditions.		
\end{itemize}
	
\subsection{Mixture of gaussians in the presence of noise}
\begin{itemize}
	\item Kevin A. Lai, Anup B. Rao, and Santosh Vempala, {\em Agnostic estimation of mean and covariance [LRV16]}\\
	Estimate the mean of a gaussian distribution when $\epsilon$ fraction of points are corrupted by noise.\\
	Propose an efficient algorithm that can estimate it with almost linear samples.
	
	\item Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart {\em Robust estimators in high dimensions without the computational intractability [DKK + 16]}\\
	Achieve error $O(\epsilon\sqrt{\log (1/\epsilon)})$ for estimating the mean of a gaussian.\\
	For mixture of gaussians, output a distribution whose total variation distance is bounded by $\tilde O(poly(k)\sqrt\epsilon)$ 
	
	\item Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and
Alistair Stewart, {\em Being robust (in high dimensions) can be practical [DKK + 17a]}\\
	Acheive an error of $\sqrt\epsilon$ for mean estimation.
	
	\item Jacob Steinhardt, Moses Charikar, and Gregory Valiant, {\em Resilience: A criterion for
learning in the presence of arbitrary outliers [SCV17]}\\
	
\end{itemize}

\section{Introduction}
\subsection*{Slide 2}
What are the different approaches to clustering?
\begin{itemize}
	\item Cost-based clustering\\
	Find minimum cost partition. $k$-means (minimize sum of squared distances), $k$-median (minimize sum of distances, centers from data), fuzzy $k$-means (aims to minimize the weighted sum of squared distances), $k$-medoids (Partitioning Around Medoids). \\
	All the algorithms are some variants of local search heuristics.
	\item Hierarchical clustering\\
	Start with every point in its own cluster. Merge till one cluster remains. Common ways to merge, single-linkage, max-linkage, avg-linkage.
	\item Spectral clustering\\
	Let $L = D^{-1/2} A D^{-1/2}$ (where $A$ is adjacency matrix). The graph can be constructed in various ways ($\epsilon$-neighbourhood, $k$-nearest neighbour, weighted fully connected graph). Find top $k$ eigenvalues of this matrix and cluster using $k$-means.\\
	There are relations between these and kernel PCA.
\end{itemize}
Some recent work on cost-based hierarchical clustering. The cost is based on trying to put similar points higher on the tree.\\

\noindent Different definitions of clustering?
\begin{itemize}
	\item Soft $k$-means: Membership is probabilistic and not hard.
\end{itemize}

\subsection*{Slide 4}
Computational complexity of clustering.
	\begin{itemize}
		\item $[$Dasgupta' 08$]$\\
		Reduction from NAE3SAT. Hard for $k=2$ and $d = n$.

		\item $[$Vattani' 09$]$\\
		Planar construction. Hard for $d=2$ and $k=n^{\epsilon}$.

		\item $[$Awasthi et. al '15$]$\\
		There exists a constant $\epsilon$ such that it is NP-Hard to approximate $k$-means within $(1+\epsilon)$. Reduction from  vertex cover on triangle free graphs which is NP-Hard to approximate within $1.36$ for $k = \Omega(n)$.

		\item $[$Megiddot and Supowit$]$\\
		The $k$-median problem as well as the $k$-center problem is NP-Hard. They even show that $k$-center is NP-Hard to approximate within a constant factor for $d = 2$ for $k =\Omega(n)$.

		\item $[$Arya et. al '01$]$\\
		The $k$-median problem has a local search based algorithm with approximation ratio $3+\epsilon$.\\
	This has been improved to $1+\sqrt{3}+\epsilon$.
	\end{itemize}

\subsection*{Slide 10-11}
Definition of structure or clusterability conditions.
	\begin{itemize}
		\item $\alpha$-center proximity\\
		$k$-means efficiently solvable under center-proximity for $\alpha > 2 + \sqrt{3}$. NP-Hard for $\alpha \le 3$.\\
		 $k$-median efficiently solvable under center-proximity for $\alpha > 1 + \sqrt{2}$. NP-Hard for $\alpha \le 2$
		 
		 \item $(\alpha, \epsilon)$-center proximity\\
		 Efficient $(1+\frac{8n}{m(C)}\epsilon)$-approximation to the $k$-median objective for $\alpha > 2 + \sqrt{7}$.
		 
		 \item $\epsilon$-separatedness\\
		 $Cost(OPT(k)) \le \epsilon^2 Cost(OPT(k-1))$. Variant of $k$-means finds a $(1+\epsilon)$-approximation with high probability in time $O(nk+k^3)$. \\
		 For $k=2$, algorithm succeeds with probability $(1-\rho)(1-\epsilon^2)(1-2\epsilon\sqrt\frac{\rho}{(1-\rho)(1-\epsilon^2)}$ where $\rho = \frac{100\epsilon^2}{1-\epsilon^2}$. Also, $r_i < \frac{\epsilon}{\sqrt{1-\epsilon^2}} d(c_i, c_j)$.  Hence, for reasonable success guarentees we need that $\epsilon^2 < \frac{1}{200}$ or $14 r_i < d(c_i, c_j)$ where $r_i$ is the average radius of a cluster.
	\end{itemize}

\subsection*{Slide 14}
Incorporating domain knowledge into clustering.
	\begin{itemize}
		\item $[$Wagstaff et. al' 01$]$\\
		A set of constraints is provided to the algorithm in the beginning. COP $k$-means is a modified version of Llyod's algorithm where you assign points to closest center such that the constraints are satisfied.
		\item $[$Ashtiani, Ben-David `15$]$\\
		The user can cluster a small subset. Learn a model consistent with that clustering. Sample complexity of learning the model. The goal is to learn a mapping under which $k$-means clustering is close to the target clustering. Prove that an ERM approach has `small' sample complexity.
		\item $[$Balcan, Blum `08$]$\\
		The oracle responds with either split a cluster or merge two clusters. Given a class $C$ of clusters, recover the target clustering. A generic but inefficient procedure which makes $O(k^3 \log |C|)$ queries.
		\item $[$Awasthi, Balcan, Voevodski '17$]$\\
		Merge-split based algorithm to recover the target clustering. If the initial clustering has atmost $\delta$ over-clustering and under-clustering error (these quantities are upper bounded by $k^2$) then the algorithm can recover target by making atmost $\delta$ merge-split queries. \\
		For correlation clustering, if the initial clustering has error $\delta$ (where $\delta$ is the number edges which are incorrect), their algorithm recovers the target by making atmost $\delta$ queries. For correlation clustering $\delta = O(n)$. Using our method, we can recover the target using only linear queries and the user doesn't need to evaluate the whole clustering everytime.
	\end{itemize}
\subsection*{Slide 22}
The proof of positive result relies on two lemmas. The first lemma says that if $d(\mu, \mu') \le \epsilon$ and $\gamma > 1 + 2\epsilon$, then $d(x, \mu') < d(y, \mu')$. Hence, there is a separation even w.r.t to the center $\mu'$.

The second lemma uses a concentration inequality in high dimensions. Namely, if $n > c\frac{\log (1/\delta)}{\epsilon^2}$ then $\|\mu - \frac{1}{n}\sum X_i\|^2 \le R^2 \epsilon$ 

\subsection*{Slide 24}
The point of the proof is the following. Any clustering of $X$ which costs $\le L$ has the following property. Atleast $m$ of rows $R_i$ are be grouped in a $A$-type clustering and the rest are grouped in $B$-type clustering. The rows of $G_i$ are singletons. And the points in $Z_i$ are added to clusters which are good for it (increase the cost by $\frac{2}{\sqrt{3}}h$. Other clusterings have cost atleast $L + \frac{w}{3}$. 

If $S_i = (s_{i1}, s_{i2}, s_{i3})$ is included in the exact cover then the row $R_i$ should be grouped in a $A$-type clustering. Then $x'_{ij}$ location will force all other dash locations to be in $B$-type clusterings (as the up ones will have to go `up' and the down ones will have to go down). Since there are $3m$ variables they will have exactly $m$ rows in $A$-type clustering. 

\subsection*{Slide 29}
Designing a distance metric.
\begin{itemize}
	\item Character-based: Edit distance and its many variants.
	\item Token-based: Word (or token) representations. Word2vec or tf-idf measures.
	\item Phonetic-based: Soundex, NYSIIS. Replace similar sounding constants with numbers or letters.
	\item Numerical
\end{itemize}
Learning a distance metric.
\begin{itemize}
	\item Learning the weights of (distance functions across) different fields.
	\item Learning a classifier over pairs. (Can view as a $\{0, 1\}$ distance function.)
\end{itemize}

\subsection*{Slide 30}
\begin{itemize}
	\item $[$Bansal et al., 2004$]$\\
	Introduce the problem of correlation clustering (or min-disagree). Give a constant factor approximation to the problem. But the constant is huge. 
	\item $[$Demaine et al., 2006$]$\\
	Gave a $4\log_2 n$ approximation to the minimization problem. Gave a $O(\log_2 n)$ approximation to the weighted correlation clustering problem. The problem is APX-Hard for general (where not all edge labels are known) weighted or unweighted graphs.
	\item $[$Charikar et al., 2005$]$\\
	Gave a factor $4$ approximation to the minimization problem. Algorithm based on convex relaxation. Also, proved that the problem is APX-Hard. 
	\item $[$Claire Mathieu and Warren Schudy, 2010$]$ and $[$Makarychev et. al' 2015$]$
	Correlation clustering under a semi-random model. \\
	A planted clustering with each of the edges flipped independently by a probability $p$. Makarychev et. al (general graphs) finds a clustering of cost $(1 +\epsilon) OPT +O((1-2p)-4\delta-3n\log^3n)$. Mathieu et. al (complete graphs) finds a clustering of cost $1+O(n^{-\frac{1}{6}})$ times the optimal. 
	\item $[$Giotis and Guruswami, 2006$]$\\
	Give a PTAS when the number of clusters is known. Running time is $n^{O(\frac{9^k}{\epsilon^2})}\log n$.
	
	\item $[$Ailon et al., 2018$]$\\
	Propose an algorithm with makes $O(\frac{k^{14} \log k\log n}{\epsilon^6})$ queries to an oracle and finds a $(1+\epsilon)$-approximation. Prove that there exists an $\epsilon$ such that any $(1+\epsilon)$-approximation makes $\Omega(\frac{k}{ploy(\log k)})$ queries. 
\end{itemize}

\subsection*{Slide 31}
Solving $L_E(C)$ does not help us solve $L_{C^*}(C)$ even under the promise that $E$ is $(\alpha, \beta)$-close to $C^*$. \\
Consider the case when $\alpha = 0$ and $\beta = \frac{1}{2}$. In this case, all the negative edges are correct and half of the positive edges are correct.\\
\begin{table}[!ht]
\centering
\def\arraystretch{1.5}
\begin{tabular}{c|c}
	All possible clusterings & Restricted to $F$\\
	\hline
	Find $C^* = \text{argmin } L_{C^*}(C)$ & $\hat C = \text{argmin } L_{C^*}(C)$ \\
	vaccous equivalent to ``Find $C^*$ & \\
	\hline 
	Find $C^* = \text{argmin } L_{E}(C)$ & $\hat C = \text{argmin } L_{E}(C)$\\
	Understanding that the optimal &\\
	solution is the desired clustering.\\
	\hline
\end{tabular}
\caption{Different correlation formulations for de-duplication. $C^*$ is the unique deduplication solution. \textbf{Slide 31}}
\end{table}
Consider the situation when all the given graph can be decomposed into cycles of length four. And it is known that half of the positive edges are correct. In this case, for every solution to $L_E(C)$ there exists a $C^*$ such that $L_{C^*}(C)$ is `bad'. 

\subsection*{Slide 32}
\begin{table}[!ht]
\centering
\def\arraystretch{1.5}
\begin{tabular}{c|c|c|c}
	& Condition positive & Condition negative&\\
	\hline
	Predicted positive &tp &fp & $precision = \frac{tp}{tp+fp}$\\
	predicted negative &fn & tn\\
	\hline
	& $recall = \frac{tp}{tp+fn}$& $specificity = \frac{fp}{fp+tn}$ &
\end{tabular}
\caption{Precision and recall table}
\end{table}
\noindent $(\alpha, \beta)$-informative says that the edges $E$ and $C^*$ have high precision and recall w.r.t each other.  

\subsection*{Slide 36}
If there exists a clustering such that all clusters have size $C$ and no negative errors then that clustering has lowest error. As it has no negative errors and any other clustering will make more positive errors than this clustering. 

\subsection*{Slide 40}
The clause edges say the following statements. The first hyper-edge says that the literals can be either true or false. The second hyper-edge says that the literals can be either true or false. The third hyper-edge says that one literal must be true.

The construction forces that in any matching the clause edges must attach to separate literals. 

\subsection*{Slide 55}
Consider all clusterings of the set where each cluster has size two except maybe one. Any tree can contain atmost one clustering from this list. Hence, any list of trees of size $s$ will contain atmost $s$ such clusterings.


\subsection*{Slide 57}
Weighted clustering is not a linkage based algorithm like min or average linkage. 
$$d(C_l, C_i \cup C_j) = \frac{n_i}{n_i + n_j} d(C_l, C_i) + \frac{n_j}{n_i+n_j}d(C_l, C_i)$$
As compared against average linkage which gives
$$d(C_l, C_i \cup C_j) = \frac{1}{2} \Big(d(C_l, C_i) + d(C_l, C_i) \Big)$$

\subsection*{Slide 71}
Non-concentrated distribution: The density at any point is atmost $c$-times the density under the uniform distribution.

\subsection*{Slide 76}
\begin{itemize}
	\item $[$Dave, 1993$]$\\
	Considered the definition of a noise prototype which implies the same objective function as ours.
	\item $[$Ben-David, Haghtalab, 2014$]$\\
	Generic procedure which transforms any metric to $d' = \min(\lambda, d)$.\\
	If there exists an algorithm which can solve the optimization problem under the truncated metric then such an algorithm is robust to noise. 		\item $[$Georgogiannis, 2016$]$\\
	Studies the breakdown properties of this function.
\end{itemize}

\subsection*{Slide 83}
All the nosy poiints in $N_1$ have a small margin of $\alpha$ w.r.t the centers. That is,
$$| \|n-\mu_i\|^2 - \|n-\mu_j\|^2| \ge \alpha$$
All the noisy points in $N_2$ are far. 
$$ \|n- x\| \ge \nu \ge \sqrt{(\delta-1)^2+1}$$ Let $n = \min_i |B_i|$ and $\epsilon = \frac{| N_1|}{n}$ and $\rho = \frac{|I|}{nk}$. If  

\begin{itemize}
  \item $\delta > 2 + \sqrt{ O(\epsilon) + \frac{2\rho k\theta(1+1/\log(|I|))^2}{d}}$ 
  \item $\alpha \ge O(\epsilon)+ \frac{2\rho k\theta(1+1/\log|I|)^2}{d}$ 
  \item $\frac{|N_2|}{n} \le \frac{\delta^2-2\delta-O(\epsilon)}{\lambda}$
\end{itemize}
\end{document}























































